cat("--- ROC AUC COMPARISON ---\n\n")
# Extract best ROC AUC for each model
logistic_auc <- cv_results_logistic |>
collect_metrics() |>
filter(.metric == "roc_auc") |>
mutate(model = "Logistic Regression")
rf_auc <- cv_results_rf |>
show_best(metric = "roc_auc", n = 1) |>
mutate(model = "Random Forest")
xgb_auc <- cv_results_xgb |>
show_best(metric = "roc_auc", n = 1) |>
mutate(model = "XGBoost")
# Combine and display
comparison_roc <- bind_rows(logistic_auc, rf_auc, xgb_auc) |>
select(model, mean, std_err) |>
arrange(desc(mean))
print(comparison_roc)
# Identify winner
winner_model <- comparison_roc$model[1]
winner_roc <- comparison_roc$mean[1]
cat("\n✓ WINNER (ROC AUC):", winner_model,
"with", round(winner_roc, 4), "\n\n")
# ----------------------------------------------------------------------------
# 2. ALL METRICS COMPARISON
# ----------------------------------------------------------------------------
cat("--- ALL METRICS COMPARISON ---\n\n")
all_metrics_comparison <- bind_rows(
collect_metrics(cv_results_logistic) |> mutate(model = "Logistic"),
collect_metrics(cv_results_rf) |> mutate(model = "Random Forest"),
collect_metrics(cv_results_xgb) |> mutate(model = "XGBoost")
) |>
select(model, .metric, mean, std_err) |>
pivot_wider(names_from = .metric,
values_from = c(mean, std_err),
names_glue = "{.metric}_{.value}")
print(all_metrics_comparison)
cat("\n")
# ----------------------------------------------------------------------------
# 3. VISUALIZATION: MODEL PERFORMANCE
# ----------------------------------------------------------------------------
cat("--- CREATING PERFORMANCE VISUALIZATIONS ---\n\n")
# Prepare data for visualization
metrics_long <- bind_rows(
collect_metrics(cv_results_logistic) |> mutate(model = "Logistic"),
collect_metrics(cv_results_rf) |> mutate(model = "Random Forest"),
collect_metrics(cv_results_xgb) |> mutate(model = "XGBoost")
)
# Plot 1: All metrics comparison
p_all_metrics <- ggplot(metrics_long, aes(x = model, y = mean, fill = model)) +
geom_col() +
geom_errorbar(aes(ymin = mean - std_err, ymax = mean + std_err),
width = 0.2, alpha = 0.7) +
facet_wrap(~.metric, scales = "free_y", ncol = 3) +
scale_fill_manual(values = c("Logistic" = pal[1],
"Random Forest" = pal[2],
"XGBoost" = pal[3])) +
labs(
title = "Model Performance Across Multiple Metrics",
subtitle = "Error bars show ±1 standard error from cross-validation",
y = "Mean Performance",
x = NULL
) +
theme_minimal(base_size = 12) +
theme(
legend.position = "none",
axis.text.x = element_text(angle = 45, hjust = 1),
strip.text = element_text(face = "bold")
)
print(p_all_metrics)
# Plot 2: ROC AUC focused comparison
p_roc_comparison <- comparison_roc |>
ggplot(aes(x = reorder(model, mean), y = mean, fill = model)) +
geom_col(width = 0.6) +
geom_errorbar(aes(ymin = mean - std_err, ymax = mean + std_err),
width = 0.2, linewidth = 1) +
geom_text(aes(label = round(mean, 3)),
vjust = -2, size = 5, fontface = "bold") +
scale_fill_manual(values = c("Logistic Regression" = pal[1],
"Random Forest" = pal[2],
"XGBoost" = pal[3])) +
scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.1)) +
labs(
title = "ROC AUC Comparison: Model Ranking Performance",
subtitle = "Higher is better - ability to rank customers by churn risk",
y = "ROC AUC",
x = NULL
) +
theme_minimal(base_size = 14) +
theme(
legend.position = "none",
panel.grid.major.x = element_blank(),
plot.title = element_text(face = "bold", size = 16)
)
print(p_roc_comparison)
# ----------------------------------------------------------------------------
# 4. STATISTICAL SIGNIFICANCE TESTING
# ----------------------------------------------------------------------------
cat("\n--- STATISTICAL SIGNIFICANCE TESTS ---\n\n")
cat("Testing if performance differences are statistically significant...\n\n")
# Extract ROC AUC for each CV fold - CORRECTED VERSION
# For tuned models, we need to filter to the BEST configuration only
# Logistic regression (no tuning - all results)
logistic_scores <- collect_metrics(cv_results_logistic, summarize = FALSE) |>
filter(.metric == "roc_auc") |>
pull(.estimate)
# Random Forest - filter to best configuration only
best_rf_config <- select_best(cv_results_rf, metric = "roc_auc")$.config
rf_scores <- collect_metrics(cv_results_rf, summarize = FALSE) |>
filter(.metric == "roc_auc", .config == best_rf_config) |>
pull(.estimate)
# XGBoost - filter to best configuration only
best_xgb_config <- select_best(cv_results_xgb, metric = "roc_auc")$.config
xgb_scores <- collect_metrics(cv_results_xgb, summarize = FALSE) |>
filter(.metric == "roc_auc", .config == best_xgb_config) |>
pull(.estimate)
# Verify all have same length
cat("Number of CV fold results:\n")
cat("Logistic:", length(logistic_scores), "\n")
cat("Random Forest:", length(rf_scores), "\n")
cat("XGBoost:", length(xgb_scores), "\n\n")
# Check if lengths match before running t-tests
if(length(logistic_scores) == length(rf_scores) &&
length(logistic_scores) == length(xgb_scores)) {
# Pairwise t-tests (paired because same CV folds)
t_xgb_vs_log <- t.test(xgb_scores, logistic_scores, paired = TRUE)
t_xgb_vs_rf <- t.test(xgb_scores, rf_scores, paired = TRUE)
t_rf_vs_log <- t.test(rf_scores, logistic_scores, paired = TRUE)
cat("Pairwise t-tests (paired by CV fold):\n\n")
cat("XGBoost vs Logistic:       p =", format.pval(t_xgb_vs_log$p.value, digits = 3), "\n")
cat("XGBoost vs Random Forest:  p =", format.pval(t_xgb_vs_rf$p.value, digits = 3), "\n")
cat("Random Forest vs Logistic: p =", format.pval(t_rf_vs_log$p.value, digits = 3), "\n\n")
# Interpret results
if(t_xgb_vs_log$p.value < 0.05 && t_xgb_vs_rf$p.value < 0.05) {
cat("✓ XGBoost is STATISTICALLY SIGNIFICANTLY better than both alternatives (p < 0.05)\n")
} else if(t_xgb_vs_log$p.value < 0.05) {
cat("✓ XGBoost is significantly better than Logistic (p < 0.05)\n")
cat("  But not significantly different from Random Forest\n")
} else if(t_xgb_vs_rf$p.value < 0.05) {
cat("✓ XGBoost is significantly better than Random Forest (p < 0.05)\n")
cat("  But not significantly different from Logistic\n")
} else {
cat("⚠ Performance differences may not be statistically significant\n")
cat("  Consider: simpler model may be preferable if performance is similar\n")
}
} else {
cat("⚠ WARNING: CV fold counts don't match - cannot perform paired t-tests\n")
cat("   Using mean comparison instead:\n\n")
cat("Mean ROC AUC:\n")
cat("Logistic:      ", round(mean(logistic_scores), 4), "\n")
cat("Random Forest: ", round(mean(rf_scores), 4), "\n")
cat("XGBoost:       ", round(mean(xgb_scores), 4), "\n\n")
cat("Standard deviation:\n")
cat("Logistic:      ", round(sd(logistic_scores), 4), "\n")
cat("Random Forest: ", round(sd(rf_scores), 4), "\n")
cat("XGBoost:       ", round(sd(xgb_scores), 4), "\n")
}
cat("\n")
# ============================================================================
# MODEL SELECTION & FINALIZATION
# ============================================================================
cat(rep("=", 80), "\n", sep = "")
cat("=== MODEL SELECTION DECISION ===\n")
cat(rep("=", 80), "\n\n", sep = "")
# Determine which model won based on ROC AUC
winner_model <- comparison_roc$model[1]
winner_roc <- comparison_roc$mean[1]
cat("Based on ROC AUC performance, we select:", winner_model, "\n")
cat("Mean ROC AUC:", round(winner_roc, 4), "\n")
cat("Standard Error:", round(comparison_roc$std_err[1], 4), "\n\n")
cat("RATIONALE:\n")
cat("- ROC AUC is our primary metric (need to rank customers by expected loss)\n")
cat("-", winner_model, "achieved the highest cross-validated ROC AUC\n")
cat("- Model will be retrained on full analysis set for deployment\n\n")
# Store the winning model for next steps
if(winner_model == "XGBoost") {
final_cv_results <- cv_results_xgb
final_wf <- wf_xgb
cat("Selected model: XGBoost (boosted trees)\n")
} else if(winner_model == "Random Forest") {
final_cv_results <- cv_results_rf
final_wf <- wf_rf
cat("Selected model: Random Forest (tree ensemble)\n")
} else {
final_cv_results <- cv_results_logistic
final_wf <- wf_logistic
cat("Selected model: Logistic Regression (baseline)\n")
}
cat("\n✓ Final model selected and ready for training\n\n")
cat(rep("=", 80), "\n", sep = "")
cat("MODEL SELECTION COMPLETE\n")
cat(rep("=", 80), "\n\n", sep = "")
# ----------------------------------------------------------------------------
# STEP 1: SELECT BEST HYPERPARAMETERS & FINALIZE WORKFLOW
# ----------------------------------------------------------------------------
cat("--- FINALIZING MODEL WITH BEST HYPERPARAMETERS ---\n\n")
# Get best hyperparameters based on ROC AUC
best_params <- select_best(final_cv_results, metric = "roc_auc")
cat("Best hyperparameters:\n")
print(best_params)
cat("\n")
# Finalize workflow with best parameters
final_wf_tuned <- final_wf |>
finalize_workflow(best_params)
cat("✓ Workflow finalized with optimal hyperparameters\n\n")
# ----------------------------------------------------------------------------
# STEP 2: TRAIN ON FULL ANALYSIS SET
# ----------------------------------------------------------------------------
cat("--- TRAINING FINAL MODEL ON COMPLETE ANALYSIS SET ---\n")
cat("Training data: All observations from 2015-2024 (N =", nrow(analysis_set), ")\n\n")
set.seed(999)
final_fit <- fit(final_wf_tuned, data = analysis_set)
cat("✓ Final model training complete\n\n")
# ----------------------------------------------------------------------------
# STEP 3: GENERATE PREDICTIONS FOR 2025
# ----------------------------------------------------------------------------
cat("--- GENERATING PREDICTIONS FOR 2025 ---\n")
cat("Assessment data: All observations from 2025 (N =", nrow(assessment_set), ")\n\n")
predictions_2025 <- augment(final_fit, new_data = assessment_set)
cat("✓ Predictions generated\n\n")
# Preview predictions
cat("Sample predictions:\n")
predictions_2025 |>
select(id, churn, .pred_class, .pred_churned, .pred_renewed, spend) |>
head(10) |>
print()
cat("\n")
# ----------------------------------------------------------------------------
# STEP 4: MODEL PERFORMANCE ON 2025 DATA
# ----------------------------------------------------------------------------
cat("--- FINAL MODEL PERFORMANCE (2025 Assessment Set) ---\n\n")
# Calculate all metrics
final_metrics <- predictions_2025 |>
metrics(truth = churn, estimate = .pred_class, .pred_churned) |>
select(.metric, .estimate)
print(final_metrics)
cat("\n")
# Confusion matrix
cat("Confusion Matrix:\n")
cm_2025 <- conf_mat(predictions_2025, truth = churn, estimate = .pred_class)
print(cm_2025)
cat("\n")
# ROC curve
cat("Creating ROC curve...\n")
p_roc <- predictions_2025 |>
roc_curve(truth = churn, .pred_churned) |>
autoplot() +
labs(
title = "ROC Curve: Final Model Performance on 2025 Data",
subtitle = paste0("AUC = ",
round(roc_auc_vec(predictions_2025$churn,
predictions_2025$.pred_churned), 3))
) +
theme_minimal(base_size = 14)
print(p_roc)
# ----------------------------------------------------------------------------
# STEP 5: CALCULATE EXPECTED LOSS & NET VALUE
# ----------------------------------------------------------------------------
cat("--- CALCULATING EXPECTED VALUE FOR EACH CUSTOMER ---\n\n")
retention_cost <- 20000
strategy_data <- predictions_2025 |>
select(id, churn, spend, .pred_churned, sector, premium, tenure) |>
mutate(
# Expected loss if customer churns
expected_loss = .pred_churned * spend,
# Net value of retention effort
# (Expected loss we prevent - Cost of intervention)
net_value = expected_loss - retention_cost,
# Should we target this customer?
target_customer = net_value > 0
) |>
# Rank by net value (highest value targets first)
arrange(desc(net_value)) |>
mutate(
rank = row_number(),
cumulative_value = cumsum(net_value)
)
cat("Sample of top-ranked customers:\n")
strategy_data |>
select(rank, id, spend, .pred_churned, expected_loss, net_value, target_customer) |>
head(15) |>
print()
cat("\n")
# ----------------------------------------------------------------------------
# STEP 6: DETERMINE OPTIMAL NUMBER OF CUSTOMERS TO TARGET
# ----------------------------------------------------------------------------
cat("--- DETERMINING OPTIMAL RETENTION CAMPAIGN SIZE ---\n\n")
# Filter to profitable targets only
target_list <- strategy_data |>
filter(target_customer == TRUE)
optimal_count <- nrow(target_list)
total_expected_value <- sum(target_list$net_value)
total_cost <- optimal_count * retention_cost
cat("RETENTION CAMPAIGN RECOMMENDATIONS:\n")
cat("=====================================\n")
cat("Number of customers to target:", optimal_count, "\n")
cat("Total retention budget:       ", scales::dollar(total_cost, prefix = "€"), "\n")
cat("Total expected value:         ", scales::dollar(total_expected_value, prefix = "€"), "\n")
cat("Expected ROI:                 ",
scales::percent(total_expected_value / total_cost, accuracy = 0.1), "\n\n")
# Break down by characteristics
cat("TARGET CUSTOMER PROFILE:\n")
target_profile <- target_list |>
summarise(
median_spend = median(spend),
mean_churn_prob = mean(.pred_churned),
median_tenure = median(tenure),
pct_premium = mean(premium == "1"),
.groups = "drop"
)
cat("- Median annual spend:     ", scales::dollar(target_profile$median_spend, prefix = "€"), "\n")
cat("- Average churn probability:", scales::percent(target_profile$mean_churn_prob, accuracy = 0.1), "\n")
cat("- Median tenure:           ", target_profile$median_tenure, "years\n")
cat("- Premium customers:       ", scales::percent(target_profile$pct_premium, accuracy = 0.1), "\n\n")
# ----------------------------------------------------------------------------
# STEP 7: VISUALIZATION - OPTIMAL TARGETING
# ----------------------------------------------------------------------------
cat("--- CREATING STRATEGY VISUALIZATIONS ---\n\n")
# Plot 1: Cumulative value curve
p_cumulative <- ggplot(strategy_data, aes(x = rank, y = cumulative_value)) +
geom_line(color = pal[2], linewidth = 1.5) +
geom_hline(yintercept = 0, linetype = "dashed", alpha = 0.5) +
geom_vline(xintercept = optimal_count,
linetype = "dashed", color = pal[5], linewidth = 1) +
annotate("text",
x = optimal_count + 50,
y = max(strategy_data$cumulative_value) * 0.5,
label = paste0("Optimal Cutoff:\n", optimal_count, " customers"),
color = pal[5], fontface = "bold", size = 4, hjust = 0) +
scale_y_continuous(labels = scales::label_dollar(prefix = "€", scale = 1e-6, suffix = "M")) +
labs(
title = "Optimization of Retention Campaign",
subtitle = paste0("Targeting the top ", optimal_count,
" customers yields €",
round(total_expected_value/1e6, 2), "M in expected value"),
x = "Number of Customers Targeted (Ranked by Expected Value)",
y = "Cumulative Net Expected Value"
) +
theme_minimal(base_size = 14) +
theme(plot.title = element_text(face = "bold"))
print(p_cumulative)
# Plot 2: Distribution of expected loss
p_expected_loss <- strategy_data |>
ggplot(aes(x = expected_loss, fill = target_customer)) +
geom_histogram(bins = 50, alpha = 0.8) +
geom_vline(xintercept = retention_cost,
linetype = "dashed", color = "red", linewidth = 1) +
annotate("text",
x = retention_cost + 5000,
y = Inf,
label = paste0("Retention Cost\n€", retention_cost/1000, "K"),
color = "red", fontface = "bold", vjust = 1.5) +
scale_x_continuous(labels = scales::label_dollar(prefix = "€", scale = 1e-3, suffix = "K"),
limits = c(0, 150000)) +
scale_fill_manual(
values = c("FALSE" = "gray70", "TRUE" = pal[3]),
labels = c("FALSE" = "Not Worth Targeting", "TRUE" = "Target for Retention")
) +
labs(
title = "Distribution of Expected Loss per Customer",
subtitle = "Only target customers where expected loss exceeds retention cost",
x = "Expected Loss (Churn Probability × Annual Spend)",
y = "Number of Customers",
fill = "Targeting Decision"
) +
theme_minimal(base_size = 14) +
theme(legend.position = "bottom")
print(p_expected_loss)
# Plot 3: Priority matrix (Risk vs Value)
p_matrix <- strategy_data |>
mutate(
risk_tier = cut(.pred_churned,
breaks = c(0, 0.3, 0.5, 0.7, 1),
labels = c("Low", "Medium", "High", "Critical")),
value_tier = cut(spend,
breaks = quantile(spend, c(0, 0.25, 0.5, 0.75, 1)),
labels = c("Q1 (Low)", "Q2", "Q3", "Q4 (High)"),
include.lowest = TRUE)
) |>
count(risk_tier, value_tier) |>
ggplot(aes(x = risk_tier, y = value_tier, fill = n, size = n)) +
geom_point(shape = 21, color = "white", stroke = 2) +
geom_text(aes(label = n), color = "white", fontface = "bold", size = 5) +
scale_fill_gradient(low = pal[3], high = pal[5], name = "# Customers") +
scale_size_continuous(range = c(10, 30), guide = "none") +
labs(
title = "Customer Prioritization Matrix: Risk × Value",
subtitle = "Focus on high-risk, high-value quadrant (top-right)",
x = "Predicted Churn Risk",
y = "Customer Value (Revenue Quartile)"
) +
theme_minimal(base_size = 14) +
theme(panel.grid = element_blank())
print(p_matrix)
# ----------------------------------------------------------------------------
# STEP 8: EXPORTABLE TARGET LIST
# ----------------------------------------------------------------------------
cat("\n--- TOP 20 CUSTOMERS FOR RETENTION OUTREACH ---\n\n")
actionable_list <- strategy_data |>
filter(target_customer == TRUE) |>
select(
Rank = rank,
Customer_ID = id,
Sector = sector,
Premium = premium,
Tenure = tenure,
Annual_Revenue = spend,
Churn_Probability = .pred_churned,
Expected_Loss = expected_loss,
Net_Value = net_value
) |>
mutate(
Annual_Revenue = scales::dollar(Annual_Revenue, prefix = "€"),
Churn_Probability = scales::percent(Churn_Probability, accuracy = 0.1),
Expected_Loss = scales::dollar(Expected_Loss, prefix = "€"),
Net_Value = scales::dollar(Net_Value, prefix = "€"),
Premium = if_else(Premium == "1", "Yes", "No")
) |>
head(20)
print(actionable_list)
cat("\n✓ This list can be exported to CSV and provided to the sales team\n\n")
# Optional: Save to CSV
# write_csv(actionable_list, "retention_targets_2025.csv")
# ----------------------------------------------------------------------------
# STEP 9: SENSITIVITY ANALYSIS
# ----------------------------------------------------------------------------
cat("--- SENSITIVITY ANALYSIS ---\n")
cat("What if our assumptions about retention cost are wrong?\n\n")
sensitivity_scenarios <- tibble(
retention_cost = seq(15000, 30000, by = 2500)
) |>
mutate(
analysis = map(retention_cost, function(cost) {
predictions_2025 |>
mutate(
expected_loss = .pred_churned * spend,
net_value = expected_loss - cost,
profitable = net_value > 0
) |>
summarise(
n_targets = sum(profitable),
total_value = sum(net_value[profitable]),
total_cost = n_targets * cost,
roi = total_value / total_cost
)
})
) |>
unnest(analysis)
print(sensitivity_scenarios)
p_sensitivity <- ggplot(sensitivity_scenarios,
aes(x = retention_cost / 1000, y = total_value / 1e6)) +
geom_line(color = pal[2], linewidth = 1.5) +
geom_point(size = 3, color = pal[2]) +
geom_vline(xintercept = 20, linetype = "dashed", color = pal[5]) +
scale_x_continuous(breaks = seq(15, 30, 2.5)) +
labs(
title = "Sensitivity Analysis: Retention Cost Assumptions",
subtitle = "Strategy remains profitable across a range of cost scenarios",
x = "Retention Cost per Customer (€ thousands)",
y = "Total Expected Value (€ Millions)"
) +
theme_minimal(base_size = 14)
print(p_sensitivity)
cat("\n")
# How reliable are the predicted probabilities?
calibration_data <- predictions_2025 |>
mutate(
prob_bin = cut(.pred_churned,
breaks = seq(0, 1, 0.1),
include.lowest = TRUE)
) |>
group_by(prob_bin) |>
summarise(
predicted_prob = mean(.pred_churned),
actual_churn_rate = mean(churn == "churned"),
n = n(),
.groups = "drop"
) |>
filter(!is.na(prob_bin))
print(calibration_data)
# How reliable are the predicted probabilities?
calibration_data <- predictions_2025 |>
mutate(
prob_bin = cut(.pred_churned,
breaks = seq(0, 1, 0.1),
include.lowest = TRUE)
) |>
group_by(prob_bin) |>
summarise(
predicted_prob = mean(.pred_churned),
actual_churn_rate = mean(churn == "churned"),
n = n(),
.groups = "drop"
) |>
filter(!is.na(prob_bin))
print(calibration_data)
# Does the model work better for certain customer types?
segment_performance <- predictions_2025 |>
mutate(
segment = case_when(
premium == "1" ~ "Premium",
tenure <= 2 ~ "New (≤2 years)",
TRUE ~ "Standard/Established"
)
) |>
group_by(segment) |>
summarise(
n = n(),
roc_auc = roc_auc_vec(churn, .pred_churned),
.groups = "drop"
) |>
arrange(desc(roc_auc))
print(segment_performance)
